{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b66009e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from llama.tokenizer import Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ea0e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003862\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def read_csv(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def read_yaml(config_path):\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return config\n",
    "\n",
    "def split_text2chunk(text, chunk_size=512):\n",
    "    '''\n",
    "    一条文本太长，进行分块\n",
    "    '''\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        yield text[i:i+chunk_size]\n",
    "\n",
    "# 加载tokenizer, text_chunks, config\n",
    "tokenizer = Tokenizer(\"./llama/tokenizer.model\")\n",
    "config = read_yaml(\"./config.yaml\")\n",
    "train_text = read_csv(\"./archive/train.csv\")\n",
    "print(len(train_text))\n",
    "print(tokenizer.pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6510c28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251283\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def split_text(text):\n",
    "    # 使用正则表达式匹配所有标点符号、换行符和制表符进行切分\n",
    "    tokens = re.findall(r\"[\\w']+|[.,!?;()\\n\\t]\", text)\n",
    "    return tokens\n",
    "word_list = split_text(train_text)\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cd5417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词后token数: 330059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntrain_chunks = list(split_text2chunk(word_list, config[\"model_config\"][\"seq_len\"]))\\ntrain_data_list = []\\nfor chunk in train_chunks:\\n    tokens = \\' \\'.join(chunk)\\n    tokenized_chunk = tokenizer.encode(tokens, False, False)\\n    train_data_list.append(tokenized_chunk)\\npadded_train_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in train_data_list], batch_first=True, padding_value = tokenizer.pad_id)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "#先tokenize，后切分成512一组----》容易保证输入seq_len一致，但可能破坏文本一致性\n",
    "train_data = tokenizer.encode(train_text, bos=False, eos=False)\n",
    "print(\"分词后token数:\", len(train_data))\n",
    "#padding and padding mask\n",
    "train_data_list = list(split_text2chunk(train_data, config[\"model_config\"][\"seq_len\"]+1))\n",
    "padded_train_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in train_data_list], batch_first=True)\n",
    "train_dataset = TextDataset(padded_train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "#先按words切分为512的chunk, 再进行tokenize---》容易保持文本一致性，但seq_len不一定相同\n",
    "\n",
    "'''\n",
    "train_chunks = list(split_text2chunk(word_list, config[\"model_config\"][\"seq_len\"]))\n",
    "train_data_list = []\n",
    "for chunk in train_chunks:\n",
    "    tokens = ' '.join(chunk)\n",
    "    tokenized_chunk = tokenizer.encode(tokens, False, False)\n",
    "    train_data_list.append(tokenized_chunk)\n",
    "padded_train_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in train_data_list], batch_first=True, padding_value = tokenizer.pad_id)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a610c4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "644\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(train_dataloader)) # 每个chunk 512个token 每个batch 32个chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e525006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model): #d_model为单个token的embedding维度\n",
    "        super(tokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class RotationEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model, device = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        super(RotationEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "        self.device = device\n",
    "    def cal_theta(self, base=10000):\n",
    "        # return shape (d_model,)\n",
    "        _2i = torch.arange(0, self.d_model, step=2, device=self.device)\n",
    "        theta = 1 / (base ** (_2i / self.d_model))\n",
    "        theta =torch.repeat_interleave(theta, repeats=2)\n",
    "        return theta\n",
    "    \n",
    "    def cal_cos_sin(self):\n",
    "        pos = torch.arange(0, self.max_len, device=self.device).unsqueeze(1)\n",
    "        # pos shape (max_len, 1)\n",
    "        angles = pos * self.cal_theta() # shape (max_len, d_model) 广播运算\n",
    "        embeddings = torch.stack([torch.cos(angles), torch.sin(angles)], dim=-1) # shape (max_len, d_model, 2)\n",
    "        #dim=-1表示在最后一维插入，变成[cos(angles), sin(angles)]\n",
    "        # shape (max_len, d_model, 2)\n",
    "        embeddings = embeddings.unsqueeze(0).unsqueeze(0) # shape (1, 1, max_len, d_model, 2)\n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, q, k):\n",
    "        # q shape (batch_size, n_head, max_len, d_head)\n",
    "        embeddings = self.cal_cos_sin()\n",
    "        #print(\"q shape:\", q.shape)\n",
    "        #print(\"k shape:\", k.shape)\n",
    "        q2 = torch.stack([-q[..., 1::2], q[..., ::2]], dim=-1)\n",
    "        q2 = q2.reshape(q.shape)\n",
    "        cos_pos = embeddings[..., 0].squeeze(-1)\n",
    "        expected_dims = [1, 1, self.max_len, self.d_model]\n",
    "        for i, expected_dim in enumerate(expected_dims):\n",
    "            assert cos_pos.size(i) == expected_dim, f\"Expected dimension {i} to be {expected_dim}, but got {cos_pos.size(i)}\"\n",
    "        sin_pos = embeddings[..., 1].squeeze(-1)\n",
    "        #print(\"sin and cos shape:\", sin_pos.shape, cos_pos.shape)\n",
    "        q = q * cos_pos + q2 * sin_pos\n",
    "\n",
    "        k2 = torch.stack([-k[..., 1::2], k[..., ::2]], dim=-1)\n",
    "        k2 = k2.reshape(k.shape)\n",
    "        # 更新kw, *对应位置相乘\n",
    "        k = k * cos_pos + k2 * sin_pos\n",
    "        return q, k\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden=2048, drop_prob=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_hidden)\n",
    "        self.linear2 = nn.Linear(d_hidden, d_model)\n",
    "        self.activate = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        if self.linear1.bias is not None:\n",
    "            nn.init.zeros_(self.linear1.bias)\n",
    "        if self.linear2.bias is not None:\n",
    "            nn.init.zeros_(self.linear2.bias)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, n_heads):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        #print(\"mutihead attention para: d_model:{}, seq_len:{}, n_heads:{}\".format(d_model, seq_len, n_heads))\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = d_model // n_heads\n",
    "        #print(\"mutihead attention para: d_model:{}, seq_len:{}, n_heads:{}, head_dim:{}\".format(d_model, seq_len, n_heads, self.head_dim))\n",
    "\n",
    "        self.values = nn.Linear(d_model, self.head_dim * n_heads, bias=False)\n",
    "        self.keys = nn.Linear(d_model, self.head_dim * n_heads, bias=False)\n",
    "        self.queries = nn.Linear(d_model, self.head_dim * n_heads, bias=False)\n",
    "        self.fc_out = nn.Linear(n_heads * self.head_dim, d_model)\n",
    "        self.RoPE = RotationEmbedding(max_len=seq_len, d_model=self.head_dim)\n",
    "        nn.init.kaiming_normal_(self.values.weight)\n",
    "        nn.init.kaiming_normal_(self.keys.weight)\n",
    "        nn.init.kaiming_normal_(self.queries.weight)\n",
    "        nn.init.xavier_normal_(self.fc_out.weight)\n",
    "        if self.fc_out.bias is not None:\n",
    "            nn.init.zeros_(self.fc_out.bias)\n",
    "        \n",
    "    def forward(self, x, mask, use_rope=True):\n",
    "        '''\n",
    "        x : (batch_size, seq_len, d_model)\n",
    "        mask : (batch_size, 1, seq_len, seq_len)\n",
    "        '''\n",
    "        #print(\"x shape:\", x.shape)\n",
    "        #print(\"mask shape:\", mask.shape)\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        values = self.values(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "        keys = self.keys(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "        queries = self.queries(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "        keys = keys.permute(0, 2, 1, 3)\n",
    "        queries = queries.permute(0, 2, 1, 3)\n",
    "        # q,k,v : (batch_size, n_heads, seq_len, d_head)\n",
    "        #print(\"queries shape:\", queries.shape)\n",
    "        if use_rope:\n",
    "            queries, keys = self.RoPE(queries, keys)\n",
    "\n",
    "        weights = torch.matmul(queries, keys.permute(0, 1, 3, 2)) #(bs, n, seq_len, d_head) @ (bs, n, d_head, seq_len) -> (bs, n, seq_len, seq_len)\n",
    "\n",
    "        if mask is not None: \n",
    "            weights = weights.masked_fill(mask == 1, float(\"-1e10\"))\n",
    "\n",
    "        attention = F.softmax(weights / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)), dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(bs, seq_len, int(self.n_heads * self.head_dim))\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x, mask, k_cache, v_cache, use_rope=True):\n",
    "        bs = x.shape[0]\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        # prompting process\n",
    "        values = self.values(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "        keys = self.keys(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "        # [B, L, H, D]\n",
    "\n",
    "        queries = self.queries(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "        # [B, L, H, D]\n",
    "\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "        keys = keys.permute(0, 2, 1, 3)\n",
    "        queries = queries.permute(0, 2, 1, 3)\n",
    "        # [B, H, L, D]\n",
    "\n",
    "        if k_cache is not None:\n",
    "            keys = torch.cat([k_cache, keys], dim=2)\n",
    "            values = torch.cat([v_cache, values], dim=2)\n",
    "            # [B, H, L+1, D]\n",
    "\n",
    "        if use_rope:\n",
    "            queries, keys = self.RoPE(queries, keys)\n",
    "\n",
    "        weights = torch.matmul(queries, keys.permute(0, 1, 3, 2))\n",
    "\n",
    "        if mask is not None:\n",
    "            weights = weights.masked_fill(mask == 1, float(\"-1e10\"))\n",
    "\n",
    "        attention = F.softmax(weights / (self.head_dim ** (1 / 2)), dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(bs, seq_len, self.n_heads * self.head_dim)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out, keys, values\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, n_heads, d_hidden, drop_prob=0.1, device = 'cuda'):\n",
    "        super().__init__()\n",
    "        #self.embed = tokenEmbedding(vocab_size=voc_size, d_model=d_model)\n",
    "        self.pre_norm = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiheadAttention(d_model, seq_len=seq_len, n_heads=n_heads)\n",
    "        self.ffn = FeedForward(d_model, d_hidden=d_hidden, drop_prob=drop_prob)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.norm_ffn = nn.LayerNorm(d_model)\n",
    "        nn.init.normal_(self.pre_norm.weight, mean=0, std=1e-2)\n",
    "        nn.init.zeros_(self.pre_norm.bias)\n",
    "        nn.init.normal_(self.norm_ffn.weight, mean=0, std=1e-2)\n",
    "        nn.init.zeros_(self.norm_ffn.bias)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        _x = x\n",
    "        x = self.pre_norm(x)\n",
    "        x = self.attn(x, mask)\n",
    "        x = self.dropout1(x)\n",
    "        x1 = (x + _x)\n",
    "        x = self.norm_ffn(x1)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout1(x)\n",
    "        return x + x1\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, pad_id, voc_size, d_model, seq_len, n_heads, n_layers, d_hidden, drop_prob=0.1, device = 'cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, seq_len, n_heads, d_hidden, drop_prob, device) for _ in range(n_layers)])\n",
    "        self.embed = tokenEmbedding(vocab_size=voc_size, d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.last_norm = nn.LayerNorm(d_model)\n",
    "        self.last_linear = nn.Linear(d_model, voc_size)\n",
    "        #self.softmax = nn.Softmax(dim=-1)\n",
    "        self.pad_id = pad_id\n",
    "        self.device = device\n",
    "        nn.init.xavier_uniform_(self.last_linear.weight)\n",
    "        nn.init.normal_(self.last_norm.weight, mean=0, std=1e-2)\n",
    "\n",
    "    \n",
    "    def create_mask(self, x):\n",
    "        #x_shape: (batch_size, seq_len)\n",
    "        #mask==True 的位置, 在注意力计算后会被忽略\n",
    "        pad_mask = (x==self.pad_id).unsqueeze(1).unsqueeze(3) #batch_size, 1, seq_len, 1\n",
    "        seq_len = x.shape[1]\n",
    "        up_tri_mask = (torch.triu(torch.ones(seq_len, seq_len)) == 1).bool().to(self.device) \n",
    "        mask = torch.logical_or(pad_mask, up_tri_mask)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mask = self.create_mask(x)\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout1(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.last_norm(x)\n",
    "        x = self.last_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e113896",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_id = tokenizer.pad_id\n",
    "voc_size = tokenizer.n_words\n",
    "d_model = config[\"model_config\"]['d_model']\n",
    "n_heads = config[\"model_config\"]['n_heads']\n",
    "d_hidden = config[\"model_config\"]['d_hidden']\n",
    "drop_prob1 = config[\"model_config\"]['drop_prob1']\n",
    "drop_prob2 = config[\"model_config\"]['drop_prob2']\n",
    "n_layers = config[\"model_config\"]['n_layers']\n",
    "seq_len = 512\n",
    "\n",
    "model = DecoderOnlyTransformer(pad_id=pad_id, d_model=d_model, n_heads=n_heads, d_hidden=d_hidden, drop_prob=drop_prob1, voc_size=voc_size, seq_len=seq_len, n_layers=n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eac395e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2983b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练总steps:16100, warm-up steps:1610\n",
      "checkpoint 路径:D:\\学习\\研1\\机器学习\\assignment3-transformer\n"
     ]
    }
   ],
   "source": [
    "#设置损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"training_config\"][\"learning_rate\"])\n",
    "\n",
    "num_epochs = 100 \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# 计算总训练步骤数和 warm-up 步骤数\n",
    "total_steps = num_epochs * len(train_dataloader)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "print(f\"训练总steps:{total_steps}, warm-up steps:{warmup_steps}\")\n",
    "\n",
    "#设置checkpoint保存路径\n",
    "import os\n",
    "checkpoint_dir = os.getcwd()\n",
    "print(f\"checkpoint 路径:{checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a24327c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|                                                                             | 0/161 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 250.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# 前向传播\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\mind-vis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\mind-vis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 226\u001b[0m, in \u001b[0;36mDecoderOnlyTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    224\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, mask)\n\u001b[0;32m    225\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_norm(x)\n\u001b[1;32m--> 226\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\mind-vis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\mind-vis\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\mind-vis\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 250.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "init_checkpoint = None\n",
    "if init_checkpoint is not None:\n",
    "    checkpoint = torch.load(init_checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "cur_steps = 0  \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for batch_data in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        # 假设 batch 包含 input_ids\n",
    "        input_ids = batch_data.to(device)\n",
    "        # 创建 labels，即 input_ids 向右偏移一个时间步\n",
    "        labels = input_ids[:, 1:].contiguous()\n",
    "        input_ids = input_ids[:, :-1].contiguous()\n",
    "        \n",
    "        # 将梯度置零\n",
    "        optimizer.zero_grad()\n",
    "        #with torch.cuda.amp.autocast():\n",
    "        # 前向传播\n",
    "        logits = model(input_ids)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # 更新学习率\n",
    "        cur_steps += 1\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 打印平均损失\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save({'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict()}, f\"checkpoint_steps{cur_steps}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c50f64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 201977 KiB | 201977 KiB | 201977 KiB |      0 B   |\n",
      "|       from large pool | 177152 KiB | 177152 KiB | 177152 KiB |      0 B   |\n",
      "|       from small pool |  24825 KiB |  24825 KiB |  24825 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 201977 KiB | 201977 KiB | 201977 KiB |      0 B   |\n",
      "|       from large pool | 177152 KiB | 177152 KiB | 177152 KiB |      0 B   |\n",
      "|       from small pool |  24825 KiB |  24825 KiB |  24825 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 201977 KiB | 201977 KiB | 201977 KiB |      0 B   |\n",
      "|       from large pool | 177152 KiB | 177152 KiB | 177152 KiB |      0 B   |\n",
      "|       from small pool |  24825 KiB |  24825 KiB |  24825 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 219136 KiB | 219136 KiB | 219136 KiB |      0 B   |\n",
      "|       from large pool | 192512 KiB | 192512 KiB | 192512 KiB |      0 B   |\n",
      "|       from small pool |  26624 KiB |  26624 KiB |  26624 KiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  17159 KiB |  18426 KiB |  66558 KiB |  49399 KiB |\n",
      "|       from large pool |  15360 KiB |  16384 KiB |  52224 KiB |  36864 KiB |\n",
      "|       from small pool |   1799 KiB |   2046 KiB |  14334 KiB |  12535 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      83    |      83    |      83    |       0    |\n",
      "|       from large pool |      14    |      14    |      14    |       0    |\n",
      "|       from small pool |      69    |      69    |      69    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      83    |      83    |      83    |       0    |\n",
      "|       from large pool |      14    |      14    |      14    |       0    |\n",
      "|       from small pool |      69    |      69    |      69    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      18    |      18    |      18    |       0    |\n",
      "|       from large pool |       5    |       5    |       5    |       0    |\n",
      "|       from small pool |      13    |      13    |      13    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       5    |       5    |      18    |      13    |\n",
      "|       from large pool |       3    |       3    |       5    |       2    |\n",
      "|       from small pool |       2    |       2    |      13    |      11    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n",
      "206824448\n",
      "206824448\n",
      "224395264\n",
      "224395264\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 获取当前 GPU 的显存使用情况\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "# 获取当前 GPU 的显存分配情况\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.max_memory_allocated())\n",
    "print(torch.cuda.memory_reserved())\n",
    "print(torch.cuda.max_memory_reserved())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b31da4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mind-vis] *",
   "language": "python",
   "name": "conda-env-mind-vis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
