{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d9925df",
   "metadata": {},
   "source": [
    "1. 导入需要用到的库，主要是pytorch，自己下载的llama tokenizer，以及训练需要用到的管理学习率和可视化的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98867b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from llama.tokenizer import Tokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6df7b-ecd4-4fd6-a4e4-cb477c7b1de1",
   "metadata": {},
   "source": [
    "2. 读取config文件，以及训练集\n",
    "\n",
    "注意：因为本任务是风格化文本生成，很难比较生成文本的质量，我干脆把train.csv和validation.csv的内容放到一起作为训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ea0e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1059631\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "def read_csv(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "    return text\n",
    "\n",
    "def read_yaml(config_path):\n",
    "    with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    return config\n",
    "\n",
    "def split_text2chunk(text, bos_id=None, eos_id=None, chunk_size=512):\n",
    "    '''\n",
    "    一条文本太长，进行分块\n",
    "    '''\n",
    "    for i in range(0, len(text), chunk_size):\n",
    "        if bos_id is not None and eos_id is not None:\n",
    "            res = [bos_id] + text[i:i+chunk_size-2] + [eos_id]\n",
    "        elif bos_id is not None and eos_id is None:\n",
    "            res = [bos_id] + text[i:i+chunk_size-1]\n",
    "        elif bos_id is None and eos_id is not None:\n",
    "            res = text[i:i+chunk_size-1] + [eos_id]\n",
    "        else:\n",
    "            res = text[i:i+chunk_size]\n",
    "        yield res\n",
    "\n",
    "# 加载tokenizer, text_chunks, config\n",
    "tokenizer = Tokenizer(\"./llama/tokenizer.model\")\n",
    "config = read_yaml(\"./config.yaml\")\n",
    "train_text = read_csv(\"./archive/train.csv\")\n",
    "print(len(train_text))\n",
    "print(tokenizer.pad_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6510c28d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "265769\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def split_text(text):\n",
    "    # 使用正则表达式匹配所有标点符号、换行符和制表符进行切分\n",
    "    tokens = re.findall(r\"[\\w']+|[.,!?;()\\n\\t]\", text)\n",
    "    return tokens\n",
    "word_list = split_text(train_text)\n",
    "print(len(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48883da7-2798-43e3-93b0-3f09c732d08f",
   "metadata": {},
   "source": [
    "3. 构建数据集(dataset)和dataloader，这里有两种选择：\n",
    "   - 先tokenize，后切分成512一组---->容易保证输入seq_len一致，但可能破坏文本一致性。\n",
    "   - 先按words语义切分为512的chunk, 再进行tokenize--->容易保持文本一致性，但seq_len不一定相同，需要截断与pad.\n",
    "\n",
    "实际上文本很长的时候，两种方法差不多。这里为了方便选择第一种"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21cd5417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词后token数: 349246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntrain_chunks = list(split_text2chunk(word_list, config[\"model_config\"][\"seq_len\"]))\\ntrain_data_list = []\\nfor chunk in train_chunks:\\n    tokens = \\' \\'.join(chunk)\\n    tokenized_chunk = tokenizer.encode(tokens, False, False)\\n    train_data_list.append(tokenized_chunk)\\npadded_train_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in train_data_list], batch_first=True, padding_value = tokenizer.pad_id)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "#先tokenize，后切分成512一组----》容易保证输入seq_len一致，但可能破坏文本一致性\n",
    "train_data = tokenizer.encode(train_text, bos=False, eos=False)\n",
    "print(\"分词后token数:\", len(train_data))\n",
    "#给每个chunk加上<bos>, <pos>\n",
    "train_data_list = list(split_text2chunk(train_data, bos_id = tokenizer.bos_id, eos_id = tokenizer.eos_id, chunk_size = config[\"model_config\"][\"seq_len\"]+1))\n",
    "\n",
    "padded_train_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in train_data_list], batch_first=True)\n",
    "train_dataset = TextDataset(padded_train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "#先按words语义切分为512的chunk, 再进行tokenize---》容易保持文本一致性，但seq_len不一定相同，需要截断与pad\n",
    "\n",
    "'''\n",
    "train_chunks = list(split_text2chunk(word_list, config[\"model_config\"][\"seq_len\"]))\n",
    "train_data_list = []\n",
    "for chunk in train_chunks:\n",
    "    tokens = ' '.join(chunk)\n",
    "    tokenized_chunk = tokenizer.encode(tokens, False, False)\n",
    "    train_data_list.append(tokenized_chunk)\n",
    "padded_train_data = torch.nn.utils.rnn.pad_sequence([torch.tensor(ids) for ids in train_data_list], batch_first=True, padding_value = tokenizer.pad_id)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3958f37-2fe4-4716-a324-bf25fd3e5c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doth she tempt: but it is I\n",
      "That, lying by the violet in the sun,\n",
      "Do as the carrion does, not as the flower,\n",
      "Corrupt with virtuous season. Can it be\n",
      "That modesty may more betray our sense\n",
      "Than woman's lightness? Having waste ground enough,\n",
      "Shall we desire to raze the sanctuary\n",
      "And pitch our evils there? O, fie, fie, fie!\n",
      "What dost thou, or what art thou, Angelo?\n",
      "Dost thou desire her foully for those things\n",
      "That make her good? O, let her brother live!\n",
      "Thieves for their robbery have authority\n",
      "When judges steal themselves. What, do I love her,\n",
      "That I desire to hear her speak again,\n",
      "And feast upon her eyes? What is't I dream on?\n",
      "O cunning enemy, that, to catch a saint,\n",
      "With saints dost bait thy hook! Most dangerous\n",
      "Is that temptation that doth goad us on\n",
      "To sin in loving virtue: never could the strumpet,\n",
      "With all her double vigour, art and nature,\n",
      "Once stir my temper; but this virtuous maid\n",
      "Subdues me quite. Even till now,\n",
      "When men were fond, I smiled and wonder'd how.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Hail to you, provost! so I think you are.\n",
      "\n",
      "Provost:\n",
      "I am the provost. What's your will, good friar?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Bound by my charity and my blest order,\n",
      "I come to visit the afflicted spirits\n",
      "Here in the prison. Do me the common right\n",
      "To let me see them and to make me know\n",
      "The nature of their crimes, that I may minister\n",
      "To them accordingly.\n",
      "\n",
      "Provost:\n",
      "I would do more than that, if more were needful.\n",
      "Look, here comes one: a gentlewoman of mine,\n",
      "Who, falling in the flaws of her own youth,\n",
      "Hath blister'd her report: she is with child;\n",
      "And he that got it, sentenced; a young man\n",
      "More fit to do another such offence\n",
      "Than die for this.\n",
      "\n",
      "DUKE VINCENTIO:\n"
     ]
    }
   ],
   "source": [
    "#验证train_dataloader经过encode之后是对的，可以decode解码\n",
    "batch_data1 = None\n",
    "for i, tmp in enumerate(train_dataloader):\n",
    "    if i==0:\n",
    "        batch_data1 = tmp\n",
    "        break\n",
    "for bd in batch_data1:\n",
    "    words = tokenizer.decode(bd.tolist())\n",
    "    break\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a610c4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "681\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(train_dataloader)) # 每个chunk 512个token 每个batch 32个chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cce867-511a-4e4f-9ec6-72dce079bd90",
   "metadata": {},
   "source": [
    "4. 构建decoder-only transformer：\n",
    "- 自适应输入长度\n",
    "- 使用pre LayerNorm代替post LayerNorm\n",
    "- 使用RoPE代替原始的正余弦位置编码\n",
    "- 使用kv-cache进行推理加速"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e525006",
   "metadata": {},
   "outputs": [],
   "source": [
    "class tokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model): #d_model为单个token的embedding维度\n",
    "        super(tokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "class RotationEmbedding(nn.Module):\n",
    "    def __init__(self, max_len, d_model):\n",
    "        super(RotationEmbedding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.d_model = d_model\n",
    "    def cal_theta(self, base=10000):\n",
    "        # return shape (d_model,)\n",
    "        _2i = torch.arange(0, self.d_model, step=2)\n",
    "        theta = 1 / (base ** (_2i / self.d_model))\n",
    "        theta =torch.repeat_interleave(theta, repeats=2)\n",
    "        return theta\n",
    "    \n",
    "    def cal_cos_sin(self):\n",
    "        pos = torch.arange(0, self.max_len).unsqueeze(1)\n",
    "        # pos shape (max_len, 1)\n",
    "        angles = pos * self.cal_theta() # shape (max_len, d_model) 广播运算\n",
    "        embeddings = torch.stack([torch.cos(angles), torch.sin(angles)], dim=-1) # shape (max_len, d_model, 2)\n",
    "        #dim=-1表示在最后一维插入，变成[cos(angles), sin(angles)]\n",
    "        # shape (max_len, d_model, 2)\n",
    "        embeddings = embeddings.unsqueeze(0).unsqueeze(0) # shape (1, 1, max_len, d_model, 2)\n",
    "        return embeddings\n",
    "    \n",
    "    def forward(self, q, k):\n",
    "        # q shape (batch_size, n_head, max_len, d_head)\n",
    "        embeddings = self.cal_cos_sin().to(q.device)\n",
    "        #print(\"q shape:\", q.shape)\n",
    "        #print(\"k shape:\", k.shape)\n",
    "        q2 = torch.stack([-q[..., 1::2], q[..., ::2]], dim=-1)\n",
    "        q2 = q2.reshape(q.shape)\n",
    "        cos_pos = embeddings[..., 0].squeeze(-1) #(1, 1, max_len, d_head)\n",
    "        \n",
    "        sin_pos = embeddings[..., 1].squeeze(-1) #(1, 1, max_len, d_head)\n",
    "        \n",
    "        q_len = q.shape[2]  #为了处理推理时的变长序列\n",
    "        k_len = k.shape[2]\n",
    "\n",
    "        q = q * cos_pos[:, :, q_len-1, :] + q2 * sin_pos[:, :, q_len-1, :]\n",
    "\n",
    "        k2 = torch.stack([-k[..., 1::2], k[..., ::2]], dim=-1)\n",
    "        k2 = k2.reshape(k.shape)\n",
    "        # 更新kw, *对应位置相乘\n",
    "        k = k * cos_pos[:, :, k_len-1, :] + k2 * sin_pos[:, :, k_len-1, :]\n",
    "        return q, k\n",
    "    \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_hidden=2048, drop_prob=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_hidden)\n",
    "        self.linear2 = nn.Linear(d_hidden, d_model)\n",
    "        self.activate = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        if self.linear1.bias is not None:\n",
    "            nn.init.zeros_(self.linear1.bias)\n",
    "        if self.linear2.bias is not None:\n",
    "            nn.init.zeros_(self.linear2.bias)\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activate(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "    \n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, n_heads):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        #print(\"mutihead attention para: d_model:{}, seq_len:{}, n_heads:{}\".format(d_model, seq_len, n_heads))\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.seq_len = seq_len\n",
    "        self.head_dim = d_model // n_heads\n",
    "        #print(\"mutihead attention para: d_model:{}, seq_len:{}, n_heads:{}, head_dim:{}\".format(d_model, seq_len, n_heads, self.head_dim))\n",
    "\n",
    "        self.values = nn.Linear(d_model, self.head_dim * n_heads, bias=False)\n",
    "        self.keys = nn.Linear(d_model, self.head_dim * n_heads, bias=False)\n",
    "        self.queries = nn.Linear(d_model, self.head_dim * n_heads, bias=False)\n",
    "        self.fc_out = nn.Linear(n_heads * self.head_dim, d_model)\n",
    "        self.RoPE = RotationEmbedding(max_len=seq_len, d_model=self.head_dim)\n",
    "        nn.init.kaiming_normal_(self.values.weight)\n",
    "        nn.init.kaiming_normal_(self.keys.weight)\n",
    "        nn.init.kaiming_normal_(self.queries.weight)\n",
    "        nn.init.xavier_normal_(self.fc_out.weight)\n",
    "        if self.fc_out.bias is not None:\n",
    "            nn.init.zeros_(self.fc_out.bias)\n",
    "        \n",
    "    def forward(self, x, mask, use_rope=True):\n",
    "        '''\n",
    "        x : (batch_size, seq_len, d_model)\n",
    "        mask : (batch_size, 1, seq_len, seq_len)\n",
    "        '''\n",
    "        bs = x.shape[0]\n",
    "\n",
    "        seq_len = x.shape[1] \n",
    "\n",
    "        values = self.values(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "        keys = self.keys(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "        queries = self.queries(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "        keys = keys.permute(0, 2, 1, 3)\n",
    "        queries = queries.permute(0, 2, 1, 3)\n",
    "        # q,k,v : (batch_size, n_heads, seq_len, d_head)\n",
    "        #print(\"queries shape:\", queries.shape)\n",
    "        if use_rope:\n",
    "            queries, keys = self.RoPE(queries, keys)\n",
    "\n",
    "        weights = torch.matmul(queries, keys.permute(0, 1, 3, 2)) #(bs, n, seq_len, d_head) @ (bs, n, d_head, seq_len) -> (bs, n, seq_len, seq_len)\n",
    "\n",
    "        if mask is not None: \n",
    "            weights = weights.masked_fill(mask != 1, float(\"-1e10\"))\n",
    "\n",
    "        attention = F.softmax(weights / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)), dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(bs, seq_len, int(self.n_heads * self.head_dim))\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "    def predict(self, x, mask, k_cache, v_cache, use_rope=True):\n",
    "        bs = x.shape[0]\n",
    "        seq_len = x.shape[1] #推理的时候应该为1\n",
    "\n",
    "        values = self.values(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "        keys = self.keys(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "        # [B, 1, H, D]\n",
    "\n",
    "        queries = self.queries(x).view(bs, seq_len, self.n_heads, self.head_dim)\n",
    "        # [B, 1, H, D]\n",
    "\n",
    "        values = values.permute(0, 2, 1, 3)\n",
    "        keys = keys.permute(0, 2, 1, 3)\n",
    "        queries = queries.permute(0, 2, 1, 3)\n",
    "        # [B, H, 1, D]\n",
    "\n",
    "        if use_rope:\n",
    "            queries, keys = self.RoPE(queries, keys)\n",
    "\n",
    "        if k_cache is not None:\n",
    "            keys = torch.cat([k_cache, keys], dim=2)\n",
    "            values = torch.cat([v_cache, values], dim=2)\n",
    "            # [B, H, L, D]\n",
    "        weights = torch.matmul(queries, keys.permute(0, 1, 3, 2))\n",
    "\n",
    "        if mask is not None:\n",
    "            weights = weights.masked_fill(mask != 1, float(\"-1e10\"))\n",
    "\n",
    "        attention = F.softmax(weights / (self.head_dim ** (1 / 2)), dim=-1)\n",
    "\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(bs, seq_len, self.n_heads * self.head_dim)\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out, keys, values\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, seq_len, n_heads, d_hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        #self.embed = tokenEmbedding(vocab_size=voc_size, d_model=d_model)\n",
    "        self.pre_norm = nn.LayerNorm(d_model)\n",
    "        self.attn = MultiheadAttention(d_model, seq_len=seq_len, n_heads=n_heads)\n",
    "        self.ffn = FeedForward(d_model, d_hidden=d_hidden, drop_prob=drop_prob)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.norm_ffn = nn.LayerNorm(d_model)\n",
    "        nn.init.normal_(self.pre_norm.weight, mean=0, std=1e-2)\n",
    "        nn.init.zeros_(self.pre_norm.bias)\n",
    "        nn.init.normal_(self.norm_ffn.weight, mean=0, std=1e-2)\n",
    "        nn.init.zeros_(self.norm_ffn.bias)\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        _x = x\n",
    "        x = self.pre_norm(x)\n",
    "        x = self.attn(x, mask)\n",
    "        x = self.dropout1(x)\n",
    "        x1 = (x + _x)\n",
    "        x = self.norm_ffn(x1)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout1(x)\n",
    "        return x + x1\n",
    "\n",
    "    def clear_cache(self):\n",
    "        self.k_cache = None\n",
    "        self.v_cache = None\n",
    "\n",
    "    def predict(self, x, mask, use_rope=True):\n",
    "        _x = x\n",
    "        x = self.pre_norm(x)\n",
    "        x, k_cache, v_cache = self.attn.predict(x, mask, self.k_cache, self.v_cache)\n",
    "        self.k_cache = k_cache\n",
    "        self.v_cache = v_cache\n",
    "        x = self.dropout1(x)\n",
    "        x1 = (x + _x)\n",
    "        x = self.norm_ffn(x1)\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout1(x)\n",
    "        return x + x1\n",
    "\n",
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    def __init__(self, bos_id, eos_id, pad_id, voc_size, d_model, seq_len, n_heads, n_layers, d_hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, seq_len, n_heads, d_hidden, drop_prob) for _ in range(n_layers)])\n",
    "        self.embed = tokenEmbedding(vocab_size=voc_size, d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.last_norm = nn.LayerNorm(d_model)\n",
    "        self.last_linear = nn.Linear(d_model, voc_size)\n",
    "        #self.softmax = nn.Softmax(dim=-1)\n",
    "        self.pad_id = pad_id\n",
    "        self.start_id = bos_id\n",
    "        self.end_id = eos_id\n",
    "        self.mode = \"train\"\n",
    "        nn.init.xavier_uniform_(self.last_linear.weight)\n",
    "        nn.init.normal_(self.last_norm.weight, mean=0, std=1e-2)\n",
    "        \n",
    "    def set_mode(self, mode):\n",
    "        if mode not in ['train', 'generate']:\n",
    "            raise ValueError(\"Unsupported mode: {}\".format(mode))\n",
    "        self.mode = mode\n",
    "    \n",
    "    def create_mask(self, x):\n",
    "        #x_shape: (batch_size, seq_len)\n",
    "        #mask==False 的位置, 在注意力计算后会被忽略\n",
    "        pad_mask = torch.ne(x, self.pad_id).unsqueeze(1).unsqueeze(3).to(x.device) #batch_size, 1, seq_len, 1\n",
    "        seq_len = x.shape[1]\n",
    "        low_tri_mask = (torch.tril(torch.ones(seq_len, seq_len)) == 1).bool().to(x.device) \n",
    "        mask = torch.logical_and(pad_mask, low_tri_mask)\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mask = self.create_mask(x)\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout1(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.last_norm(x)\n",
    "        x = self.last_linear(x)\n",
    "        return x\n",
    "\n",
    "    def clear_cache(self):\n",
    "        for layer in self.decoder_layers:\n",
    "            layer.clear_cache()\n",
    "            \n",
    "    def generate(self, x, max_len=512, use_rope=True):\n",
    "        mask = self.create_mask(x)\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout1(x)\n",
    "        for layer in self.decoder_layers:\n",
    "            x= layer.predict(x, mask, use_rope=use_rope)\n",
    "        x = self.last_norm(x)\n",
    "        x = self.last_linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e113896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置超参， config的有些超参没用上，不过无所谓\n",
    "pad_id = tokenizer.pad_id\n",
    "start_id = tokenizer.bos_id\n",
    "end_id = tokenizer.eos_id\n",
    "voc_size = tokenizer.n_words\n",
    "d_model = config[\"model_config\"]['d_model']         #512\n",
    "n_heads = config[\"model_config\"]['n_heads']         #8\n",
    "d_hidden = config[\"model_config\"]['d_hidden']       #2048\n",
    "drop_prob1 = config[\"model_config\"]['drop_prob1']   #0.2\n",
    "drop_prob2 = config[\"model_config\"]['drop_prob2']   #0.4\n",
    "n_layers = config[\"model_config\"]['n_layers']       #6\n",
    "seq_len = config[\"data_config\"][\"max_seq_length\"]   #512\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a798a2f8-4b24-4462-8ec9-21cb60f724bc",
   "metadata": {},
   "source": [
    "5. 进行训练\n",
    "- 设置device, criterion(交叉熵), optimizer(Adam)\n",
    "- 采用teacher-forcing 和 schedule sampling 两种不同的训练方式分别训练\n",
    "- 选择适当的初始化方案，如Xavier and Kaiming initialization\n",
    "- 每1k个step保存一次checkpoint\n",
    "- 采用线性学习率warm_up\n",
    "- 注：实际训练需要单卡有12G显存，没有采用混合精度训练。\n",
    "- 训练时，由于不知道epoch应该设置多少最好，可以采用保存checkpoint的方式多次训练，直到认为结果够好。或者自己用perplexity/BLEU评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2983b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teacher-forcing checkpoint 路径:/home/liyihang/assignment3-transformer\n"
     ]
    }
   ],
   "source": [
    "#设置损失函数和优化器\n",
    "model = DecoderOnlyTransformer(bos_id = start_id, eos_id = end_id, pad_id=pad_id, d_model=d_model, n_heads=n_heads, d_hidden=d_hidden, drop_prob=drop_prob1, voc_size=voc_size, seq_len=seq_len, n_layers=n_layers)\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"training_config\"][\"learning_rate\"]*10, weight_decay=config[\"training_config\"][\"weight_decay\"])\n",
    "\n",
    "#设置checkpoint保存路径\n",
    "import os\n",
    "checkpoint_dir = os.getcwd()\n",
    "print(f\"teacher-forcing checkpoint 路径:{checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54d3b64-bd68-40a2-8f3d-3420cbfe2f6c",
   "metadata": {},
   "source": [
    "5.1 teacher-forcing方式训练\n",
    "\n",
    "- 优点：能够迅速收敛；训练速度快；可以并行\n",
    "- 缺点：模型的能力依赖数据集，泛化性差（只学习预测数据集的下一个token）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a24327c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练总steps:1100, warm-up steps:110\n",
      "Epoch 1/50, Loss: 4.0771\n",
      "Epoch 2/50, Loss: 4.0690\n",
      "Epoch 3/50, Loss: 4.0768\n",
      "Epoch 4/50, Loss: 4.0779\n",
      "Epoch 5/50, Loss: 4.0786\n",
      "Epoch 6/50, Loss: 4.0694\n",
      "Epoch 7/50, Loss: 4.0773\n",
      "Epoch 8/50, Loss: 4.0787\n",
      "Epoch 9/50, Loss: 4.0768\n",
      "Epoch 10/50, Loss: 4.0828\n",
      "Epoch 11/50, Loss: 4.0733\n",
      "Epoch 12/50, Loss: 4.0754\n",
      "Epoch 13/50, Loss: 4.0738\n",
      "Epoch 14/50, Loss: 4.0773\n",
      "Epoch 15/50, Loss: 4.0829\n",
      "Epoch 16/50, Loss: 4.0730\n",
      "Epoch 17/50, Loss: 4.0853\n",
      "Epoch 18/50, Loss: 4.0743\n",
      "Epoch 19/50, Loss: 4.0762\n",
      "Epoch 20/50, Loss: 4.0721\n",
      "Epoch 21/50, Loss: 4.0773\n",
      "Epoch 22/50, Loss: 4.0843\n",
      "Epoch 23/50, Loss: 4.0771\n",
      "Epoch 24/50, Loss: 4.0728\n",
      "Epoch 25/50, Loss: 4.0804\n",
      "Epoch 26/50, Loss: 4.0786\n",
      "Epoch 27/50, Loss: 4.0752\n",
      "Epoch 28/50, Loss: 4.0789\n",
      "Epoch 29/50, Loss: 4.0782\n",
      "Epoch 30/50, Loss: 4.0739\n",
      "Epoch 31/50, Loss: 4.0738\n",
      "Epoch 32/50, Loss: 4.0746\n",
      "Epoch 33/50, Loss: 4.0744\n",
      "Epoch 34/50, Loss: 4.0768\n",
      "Epoch 35/50, Loss: 4.0786\n",
      "Epoch 36/50, Loss: 4.0728\n",
      "Epoch 37/50, Loss: 4.0752\n",
      "Epoch 38/50, Loss: 4.0777\n",
      "Epoch 39/50, Loss: 4.0735\n",
      "Epoch 40/50, Loss: 4.0774\n",
      "Epoch 41/50, Loss: 4.0756\n",
      "Epoch 42/50, Loss: 4.0762\n",
      "Epoch 43/50, Loss: 4.0754\n",
      "Epoch 44/50, Loss: 4.0691\n",
      "Epoch 45/50, Loss: 4.0794\n",
      "Epoch 46/50, Loss: 4.0778\n",
      "Epoch 47/50, Loss: 4.0815\n",
      "Epoch 48/50, Loss: 4.0787\n",
      "Epoch 49/50, Loss: 4.0746\n",
      "Epoch 50/50, Loss: 4.0736\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "# 计算总训练步骤数和 warm-up 步骤数\n",
    "num_epochs = 50\n",
    "total_steps = num_epochs * len(train_dataloader)\n",
    "warmup_steps = int(0.1 * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "print(f\"训练总steps:{total_steps}, warm-up steps:{warmup_steps}\")\n",
    "\n",
    "init_checkpoint = \"checkpoint_steps_8800.pth\"\n",
    "save_interval = 500\n",
    "\n",
    "#可以设置init checkpoint加载预训练模型\n",
    "init_steps = 8800\n",
    "if init_checkpoint is not None:\n",
    "    checkpoint = torch.load(init_checkpoint)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    \n",
    "cur_steps = 0  \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for idx, batch_data in enumerate(train_dataloader):\n",
    "        # 假设 batch 包含 input_ids\n",
    "        input_ids = batch_data.to(device)\n",
    "        # 创建 labels，即 input_ids 向右偏移一个时间步\n",
    "        labels = input_ids[:, 1:].contiguous()\n",
    "        input_ids = input_ids[:, :-1].contiguous()\n",
    "        \n",
    "        # 将梯度置零\n",
    "        optimizer.zero_grad()\n",
    "        #with torch.cuda.amp.autocast():\n",
    "        # 前向传播\n",
    "        logits = model(input_ids)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # 更新学习率\n",
    "        cur_steps += 1\n",
    "        running_loss += loss.item()\n",
    "        writer.add_scalar('Loss/train/iter', loss.item(), cur_steps)\n",
    "        if cur_steps % save_interval == 0 and cur_steps+init_steps>=3000:\n",
    "            # 保存模型\n",
    "            torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict()}, f\"checkpoint_steps_{init_steps + cur_steps}.pth\")\n",
    "    # 打印平均损失\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    writer.add_scalar('Loss/train/epoch', avg_loss, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict()}, f\"checkpoint_steps_{init_steps + cur_steps}.pth\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf5175f-14da-41c9-9896-cc1de636324f",
   "metadata": {},
   "source": [
    "5.2 使用schedule sampling训练\n",
    "\n",
    "schedule sampling是结合了autogressive和teacher-forcing的方法，在刚开始的时候大概率选择teacher-foring，等到模型有一定能力后，autogressive的概率逐渐增加。\n",
    "\n",
    "- 优点：提高模型自身的能力，不一味模仿数据集，同时容易收敛\n",
    "- 缺点：无法并行，训练速度慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42070e3c-3cf1-44ac-a5c5-c4e8163ea9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint 路径:/home/liyihang/assignment3-transformer\n",
      "3.378166897674838e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████████████████████████████████████████████████████████████████████████| 22/22 [00:15<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 136.9066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████████████████████████████████████████████████████████████████████████| 22/22 [00:14<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100, Loss: 131.7287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████████████████████████████████████████████████████████████████████████| 22/22 [00:15<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100, Loss: 129.5599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████████████████████████████████████████████████████████████████████████| 22/22 [00:15<00:00,  1.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100, Loss: 127.2383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████████████████████████████████████████████████████████████████████████| 22/22 [00:15<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100, Loss: 125.3461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████████████████████████████████████████████████████████████████████████| 22/22 [00:15<00:00,  1.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100, Loss: 123.7448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████████████████████████████████████████████████████████████████████████| 22/22 [00:15<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100, Loss: 122.3723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████████████████████████████████████████████████████████████████████████| 22/22 [00:15<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100, Loss: 122.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████████████████████████████████████████████████████████████████████████| 22/22 [00:15<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100, Loss: 120.3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:16<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100, Loss: 119.8143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:16<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100, Loss: 118.9688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:16<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100, Loss: 119.7972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:16<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100, Loss: 118.4105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:17<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100, Loss: 118.3456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:17<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100, Loss: 117.2182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:17<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100, Loss: 117.7356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:17<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100, Loss: 117.2710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:17<00:00,  1.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100, Loss: 116.3906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:18<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100, Loss: 116.3373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:18<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100, Loss: 116.2455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:18<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100, Loss: 116.4420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:18<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100, Loss: 116.4266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:19<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100, Loss: 115.9074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:19<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100, Loss: 116.4256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:19<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100, Loss: 115.8560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:19<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100, Loss: 115.6485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100, Loss: 115.3604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100, Loss: 115.9419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100, Loss: 114.8551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100, Loss: 114.7445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100, Loss: 114.7483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100, Loss: 114.7572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100, Loss: 114.6760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100, Loss: 114.4501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100, Loss: 113.9925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100, Loss: 113.8208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100, Loss: 113.9838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100, Loss: 113.6670\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100, Loss: 113.3134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:20<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100, Loss: 113.3082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100, Loss: 113.1152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100, Loss: 112.8059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100, Loss: 112.9387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100, Loss: 112.5281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100, Loss: 112.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:22<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100, Loss: 112.2702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100, Loss: 111.9103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100, Loss: 111.8032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100, Loss: 111.4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100, Loss: 111.5983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100, Loss: 111.6070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100, Loss: 111.2445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100, Loss: 111.0337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/100, Loss: 110.8757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/100, Loss: 110.5671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100, Loss: 110.7327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100, Loss: 110.4169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100, Loss: 110.4374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100, Loss: 110.0537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100, Loss: 110.0495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100, Loss: 110.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100, Loss: 109.8302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/100, Loss: 109.5994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/100, Loss: 109.5268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/100, Loss: 109.3336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100, Loss: 109.3174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/100, Loss: 109.1846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/100, Loss: 109.0942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:22<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/100, Loss: 108.9846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/100, Loss: 109.0313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/100, Loss: 108.7109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100, Loss: 108.5919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/100, Loss: 108.7141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/100, Loss: 108.5627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/100, Loss: 108.5540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/100, Loss: 108.4532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100, Loss: 108.2365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100, Loss: 108.2825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/100, Loss: 108.2203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/100, Loss: 108.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/100, Loss: 108.0536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100, Loss: 107.8493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/100, Loss: 107.9263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100, Loss: 107.7930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/100, Loss: 107.7153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/100, Loss: 107.7961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/100, Loss: 107.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100, Loss: 107.5762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100, Loss: 107.6913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/100, Loss: 107.4574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/100, Loss: 107.4857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/100, Loss: 107.4202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/100, Loss: 107.5108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/100, Loss: 107.3265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/100, Loss: 107.2343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100, Loss: 107.2855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100, Loss: 107.2108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100, Loss: 107.3578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/100: 100%|█████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/100, Loss: 107.2372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/100: 100%|████████████████████████████████████████████████████████████████████████████| 22/22 [00:21<00:00,  1.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/100, Loss: 107.2720\n"
     ]
    }
   ],
   "source": [
    "def sig(k, x):\n",
    "    return k/(k+ np.exp(x/k))\n",
    "\n",
    "def sigmoid_decay(epoch_num, decay_ratio, is_continue=False):\n",
    "    if epoch_num<15 and is_continue==False:\n",
    "        return 1.0\n",
    "    return sig(decay_ratio, epoch_num)\n",
    "\n",
    "# 计算总训练步骤数和 warm-up 步骤数\n",
    "num_epochs = 100\n",
    "\n",
    "#设置checkpoint保存路径\n",
    "checkpoint_dir = os.getcwd()\n",
    "print(f\"checkpoint 路径:{checkpoint_dir}\")\n",
    "\n",
    "#导入预训练模型 如果不想导入 设为None. \n",
    "init_checkpoint = 'sam_checkpoint_steps8800.pth'\n",
    "init_steps = 8800\n",
    "is_continue = False\n",
    "prev_tf_ratio = 1.0\n",
    "model_sam = DecoderOnlyTransformer(bos_id = start_id, eos_id = end_id, pad_id=pad_id, d_model=d_model, n_heads=n_heads, d_hidden=d_hidden, drop_prob=drop_prob1, voc_size=voc_size, seq_len=seq_len, n_layers=n_layers)\n",
    "model_sam.to(device)\n",
    "if init_checkpoint is not None:\n",
    "    is_continue = True\n",
    "    checkpoint = torch.load(init_checkpoint)\n",
    "    model_sam.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    prev_tf_ratio = 1.0 if checkpoint.get('teacher_ratio')==None else checkpoint.get('teacher_ratio')\n",
    "    print(prev_tf_ratio)\n",
    "total_steps = num_epochs * len(train_dataloader)\n",
    "warmup_steps = 0\n",
    "if is_continue==False:\n",
    "    warmup_steps = int(0.1 * total_steps)\n",
    "    print(f\"训练总steps:{total_steps}, warm-up steps:{warmup_steps}\")\n",
    "\n",
    "#设置优化器和迭代器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_sam.parameters(), lr=config[\"training_config\"][\"learning_rate\"], weight_decay=config[\"training_config\"][\"weight_decay\"])\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "writer_sam = SummaryWriter()\n",
    "cur_steps = 0  \n",
    "decay_ratio = 8\n",
    "\n",
    "#开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    model_sam.train()\n",
    "    running_loss = 0.0\n",
    "    batch_loss = 0.0\n",
    "    for batch_data in tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        \n",
    "        input_ids = batch_data.to(device) # (batch_size, seq_len)\n",
    "        loss = 0.0\n",
    "        for t in range(len(input_ids)):\n",
    "            #t时刻, 输入大小为 (batch_size, t)\n",
    "            \n",
    "            if t==0 or random.uniform(0, 1) < prev_tf_ratio:\n",
    "                #teacher-forcing, t时刻输入是input[0:t+1], 输出和input[1:t+2]作比较\n",
    "                if t == 0:\n",
    "                    output_logits = model_sam(input_ids[:, 0].unsqueeze(1))\n",
    "                    labels = input_ids[:, 1].contiguous()\n",
    "                    loss += criterion(output_logits.view(-1, output_logits.size(-1)), labels)\n",
    "                else:\n",
    "                    output_logits = model_sam(input_ids[:, :t+1])    #(batch_size, seq_len, vocab_size)\n",
    "                    labels = input_ids[:, 1:t+2].contiguous()\n",
    "                    loss += criterion(output_logits.view(-1, output_logits.size(-1)), labels.view(-1)) #输入[0, t-1]对应的gt为[1,t]\n",
    "            else:\n",
    "                #schedule-sampling, t时刻输入是input[0:t] + output[t], 输出和input[1:t+2]作比较\n",
    "                token_logits = model_sam(input_ids[:, :t])  #(bs, t, vocab_sz)\n",
    "                cur_token = torch.argmax(token_logits, dim=-1) #t时刻的预测token (bs, t)\n",
    "                cur_token = cur_token[:,-1].unsqueeze(1) #(bs,1)\n",
    "                \n",
    "                input_sample = torch.cat([input_ids[:, :t], cur_token], dim=1) #(bs, t+1)\n",
    "\n",
    "                output_logits = model_sam(input_sample)\n",
    "                labels = input_ids[:, 1:t+2].contiguous()\n",
    "                loss += criterion(output_logits.view(-1, output_logits.size(-1)), labels.view(-1))\n",
    "                \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        #batch_loss += loss.item()\n",
    "        prev_tf_ratio = sigmoid_decay(epoch, decay_ratio, is_continue) #更新teacher_forcing\n",
    "        scheduler.step()                      # 更新学习率\n",
    "\n",
    "        cur_steps += 1\n",
    "        writer_sam.add_scalar('Loss/train/iter', loss.item(), cur_steps)\n",
    "        if cur_steps % 500 == 0:\n",
    "            torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "                       'teacher_ratio':prev_tf_ratio}, f\"sam_checkpoint_steps{cur_steps+init_steps}.pth\")\n",
    "            \n",
    "    \n",
    "    # 打印平均损失\n",
    "    avg_loss = running_loss / len(train_dataloader)\n",
    "    writer_sam.add_scalar('Loss/train/epoch', avg_loss, epoch)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "# 保存模型\n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(), 'teacher_ratio':prev_tf_ratio}, f\"sam_checkpoint_steps{cur_steps + init_steps}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee79a61e-3fea-495d-a036-e545007061b5",
   "metadata": {},
   "source": [
    "6. 可视化\n",
    "使用tensorboard记录每个iteration的loss和每个epoch的平均loss，这里每个epoch差不多有22个iteration\n",
    "\n",
    "对应的图片可以在teacher_forcing_Loss_train_iter.svg, teacher_forcing_Loss_train_epoch.svg中查看。\n",
    "\n",
    "其次，在训练时可以直接在命令行输入tensorboard --logdir=runs 进行查看; 推荐在vscode中运行该笔记，可以直接查看。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318f678-cc8b-4784-9832-b71e52e1f311",
   "metadata": {},
   "source": [
    "7. 推理\n",
    "\n",
    "transformer的推理方法：\n",
    "- greedy\n",
    "- beam-search\n",
    "- top-k\n",
    "- top-p\n",
    "\n",
    "这里选择top-k方案。具体如代码注释所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c50f64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def choose_token(logits, top_k = 50):\n",
    "    \"\"\"\n",
    "    选择logits中top_k个最大的token, 计算softmax后以此为权重随机采样\n",
    "    logits: (1, 1, vocab_sz)\n",
    "    \"\"\"\n",
    "    values, indices = torch.topk(logits, top_k, dim = -1) #(1, top_k)\n",
    "    values = F.softmax(values, dim = -1)  #(1, top_k)\n",
    "    token_idx = random.choices(indices[0].tolist(), weights=values[0].tolist()) \n",
    "    return token_idx[0] #(bs, 1)\n",
    "\n",
    "\n",
    "#如果多次使用，先清除kv cache (model.clear_cache)\n",
    "def generate_poem(model, model_checkpoint_path, instruction, max_seq_len = 512):\n",
    "    #推理环节\n",
    "    #如果cuda out of memory 尝试torch.cuda.empty_cache()\n",
    "    #load model:\n",
    "    checkpoint = torch.load(model_checkpoint_path)\n",
    "    model.eval()\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    #前面的cell已经把model转移到device上了\n",
    "    #进行文本生成\n",
    "    max_seq_len = 512\n",
    "    input_ids = torch.tensor(tokenizer.encode(instruction, bos=True, eos=False)).unsqueeze(0) #(1, len)\n",
    "    model.clear_cache()\n",
    "    \n",
    "    logits = model.generate(input_ids.to(device), use_rope=True)\n",
    "    #print(logits.shape) #[1, 5, 32000]\n",
    "    \n",
    "    next_token_idx = choose_token(logits[:, -1, :],top_k = 50) #int\n",
    "    \n",
    "    input_ids = torch.cat([input_ids, torch.tensor([next_token_idx]).unsqueeze(0)], dim=1) #t+1步的输入\n",
    "    \n",
    "    initial_len = input_ids.shape[1]\n",
    "    \n",
    "    for t in range(0, max_seq_len-initial_len):\n",
    "        #由于启用了kv-cache, 每次只需要输入一个token\n",
    "        logits = model.generate(input_ids[:,-1].unsqueeze(0).to(device),use_rope=True)\n",
    "        \n",
    "        next_token_idx = choose_token(logits[:, -1, :],top_k = 50)                    #每次新生成一个单词，从概率最高的前50个单词里选\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([next_token_idx]).unsqueeze(0)], dim=1)   #t+1步的输入\n",
    "        if next_token_idx == tokenizer.eos_id:                       #遇到终止符就停止输出\n",
    "            break\n",
    "    \n",
    "    output = tokenizer.decode(input_ids.cpu().tolist())\n",
    "    print(output[0])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd077901-eaa8-4f40-820b-0f366acab5c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sir,\n",
      "And yet; come at King Aumerlemen, let me,\n",
      "Your voices;\n",
      "\n",
      "PETRUCHELLO,\n",
      "\n",
      "My soul\n",
      "Away on my lord,\n",
      "Whose the cause.\n",
      "I may\n",
      "HORTENSIO:\n",
      "CAPULIETRUCHIO:\n",
      "And take thee from Edward's face, and thou art too?\n",
      "\n",
      "\n",
      "If they be so much.\n",
      "With two of the other! What was not well for her son will she is:\n",
      "If I's:\n",
      "Theirrah, he was a kindred of God'd with the warrine\n",
      "\n",
      "\n",
      "The Duke of their good news?\n",
      "In those that do my brother'll bears that I\n",
      "PETRUCHEven with a poor Clarence, here.\n",
      "Thou hast thou art thou darest\n",
      "Theirator:\n",
      "HARD IV:\n",
      "A:\n",
      "As true man,\n",
      "That here was been so long-morrow, if thou hast\n",
      "The king.\n",
      "\n",
      "A:\n",
      "Ay me the lamentation!\n",
      "The gracious sister:\n",
      "I know it for your brother,\n",
      "And when she in that I hear the world did make my true lordly for all to keep them to thy frown me so? I had, by the people,\n",
      "And I may be so;\n",
      "This day,\n",
      "\n",
      "The very well:\n",
      "Godeign and\n",
      "As it will be like a man;\n",
      "I am\n",
      "\n",
      "CENTIO:\n",
      "Of his eyes\n",
      "No, I will not.\n",
      "\n",
      "Whose, go with his life should be found my tongue,\n",
      "DUCHIO:\n",
      "Tis hinder the sea,\n",
      "But the people.\n",
      "\n",
      "GLOUpon this the gods;\n",
      "BENEN ELIETER:\n",
      "\n",
      "\n",
      "His grace:\n",
      "IUSCORKINGHARD:\n",
      "\n",
      "\n",
      "K:\n",
      "With all the earth a subject, sir!\n",
      "S:\n",
      "How now is your brother'd.\n",
      "\n",
      "Ay sir;\n",
      "Nur wilt give me.\n",
      "You are.\n",
      "\n",
      "Is all such be hollowness have been,\n",
      "First Senator: but\n",
      "And yet for a thing, my heart as mine own lord.\n",
      "But in my power.\n",
      "To be a man?\n",
      "My lord,\n",
      "For all;\n",
      "\n",
      "You have we be at our own deeds, and I am content\n"
     ]
    }
   ],
   "source": [
    "#对teacher-forcing模型进行评测\n",
    "model = DecoderOnlyTransformer(bos_id = start_id, eos_id = end_id, pad_id=pad_id, d_model=d_model, n_heads=n_heads, d_hidden=d_hidden, drop_prob=drop_prob1, voc_size=voc_size, seq_len=seq_len, n_layers=n_layers)\n",
    "model.to(device)\n",
    "model_checkpoint_path = 'checkpoint_steps_9800.pth'\n",
    "instruction = \"Sir,\"\n",
    "output_poem = generate_poem(model, model_checkpoint_path, instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82b50e3b-5143-45c8-82dd-040f4206dacc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sir, then no longer of the sun.\n",
      "If we may\n",
      "As he should stand hatches my Lord:\n",
      "A:\n",
      "\n",
      "With wicked's\n",
      "MENES:\n",
      "'dainst the ground. I would call you are he did not,\n",
      "And bring\n",
      "To take you know, here.\n",
      "T:\n",
      "\n",
      "O, but how the king, not.\n",
      "Lie to the heavensio but thou think it out of love.\n",
      "\n",
      "A:\n",
      "And with\n",
      "If I will be gone,\n",
      "No, you have your\n",
      "\n",
      "BROMEO:\n",
      "\n",
      "\n",
      "To use I swear'T:\n",
      "\n",
      "\n",
      "\n",
      "Why must be, which if thou in them a thing. Be satisfied.\n",
      "FRIVERSICHisure with thy country, as thou'er:\n",
      "\n",
      "ILIET:\n",
      "\n",
      "He have you be married.\n",
      "IUS:\n",
      "\n",
      "God save mine.\n",
      "\n",
      "\n",
      "\n",
      "NORKATHAR:\n",
      "But, it?\n",
      "\n",
      "\n",
      "PETRUCHESS OF YORK:\n",
      "\n",
      "My father?\n",
      "I should meet at least of my lord,\n",
      "\n",
      "In mine and I thank you do repass and not the night:\n",
      "Bark.\n",
      "H:\n",
      "MERRY VI\n",
      "SICINIOLY:\n",
      "\n",
      "Nurse!\n",
      "\n",
      "Whom? what means on\n",
      "And then?\n",
      "And yet I do not so,\n",
      "Of mine?\n",
      "Than death,\n",
      "\n",
      "No,\n",
      "I know it! or that is a word, a thousand men's all to be, what, this, and more than that thou wilt speak.\n",
      "K:\n",
      "And that all this,\n",
      "And this, a man; my lord,\n",
      "\n",
      "O:\n",
      "Farewell:\n",
      "Of all the first:\n",
      "\n",
      "Or that did this I will be the poor brother, here.\n",
      "HAM:\n",
      "\n",
      "And the man and see our general's. If you.\n",
      "FIDI fear is the matter from me; yet in the duke.\n",
      "My life,\n",
      "What may speak on him to that would the slain.\n",
      "How now is so be good Camillo would be no better, I am in him and what a\n",
      "So,\n",
      "He shall have been more of such a father in,\n",
      "As I do:\n",
      "Or when I never may do me:\n",
      "\n",
      "\n",
      "And when he were a fain\n"
     ]
    }
   ],
   "source": [
    "modelsam_checkpoint_path = 'sam_checkpoint_steps11000.pth'\n",
    "instruction = \"Sir,\"\n",
    "model_sam = DecoderOnlyTransformer(bos_id = start_id, eos_id = end_id, pad_id=pad_id, d_model=d_model, n_heads=n_heads, d_hidden=d_hidden, drop_prob=drop_prob1, voc_size=voc_size, seq_len=seq_len, n_layers=n_layers)\n",
    "model_sam.to(device)\n",
    "output_p = generate_poem(model_sam, modelsam_checkpoint_path, instruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324b3d9c-749a-4907-b39d-d67ec67a8628",
   "metadata": {},
   "source": [
    "7. FLOPS的计算\n",
    "\n",
    "我们计算batch_size=1的情形，然后推广即可。\n",
    "\n",
    "一些前置知识：(H, I) * (I, W) --> (H, W), FLOPs 为 H\\*W\\*2I (如果没有bias, 就是H\\*W\\*(2I-1))\n",
    "\n",
    "前向传播：\n",
    "- Embedding 层： (seq_len, 1) * (seq_len, d_model) --> (seq_len, d_model) 这一步只需要查表，所以没有FLOPs\n",
    "- dropout层：  推理时会被禁用，忽略\n",
    "- Attention层：\n",
    "    - (seq_len, d_model) --> 首先经过三个线性层，nn.Linear(d_model, d_model) , 得到$3*(2d_m-1)*d_ms$\n",
    "    -  RoPE， 实际上是(n_head, seq_len, d_head) 和 (1, seq_len, d_head)的逐元素乘法， 先广播后计算，得到$2*s*d_m$ --->利用了[$d_h*n = d_m$]\n",
    "    -  $a = qk^T$: (n_head, seq_len, d_head) @ (n_head, d_head, seq_len) --> (n, seq_len, seq_len)，得到$2s^2d_m$\n",
    "    -  $o = a/\\sqrt(d_m)·v$: (n, seq_len, seq_len) @ (n, seq_len, d_head) --> (n, seq_len, d_head)，得到$2s^2d_m + n_hs^2$ (softmax只计算s次，忽略不计）\n",
    "    -  最后一个线性层：$2sd_m^2$, 得到输出大小为(seq_len, d_m)\n",
    "    -  综上所述，attention层的FLOPs约为$10d_m^2s + 2sd_m+n_hs^2$\n",
    "- FFN层：\n",
    "    - (seq_len, d_m)-->(seq_len, d_ffn)-->(seq_len,d_m) 主要是两个线性层的FLOPs, 约为$4d_md_fs$\n",
    "- LayerNorm层：\n",
    "    - 只在d_model上做计算(求平均值和方差)，但会对每一个元素应用此操作，复杂度为$O(sd_m)$\n",
    "- Decoder-layer：\n",
    "    - layernorm + attention + FFN层即为Decoder-layer层的FLOPs，以注意力头n=8, 隐藏层维度$d_f=4d_m$计算，得到$26sd^2 + 2ds+8s^2$\n",
    "- 输出层:\n",
    "    - 一个FFN， 负责把模型维度d映射到词表维度V上， FLOPs=$2shV$\n",
    "\n",
    "- 总和：\n",
    "    - 6个Decoder-layer + Output layer = $(26sd^2 + 2ds+8s^2)*6+2shV$\n",
    "    - 带入s=d=512, V=35000, 得到39303774208，约为$3.93*10^{10}$\n",
    "\n",
    "反向传播：\n",
    "- 一般认为，反向传播的计算量是前向传播的2倍"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f92c2054-2934-49f4-9265-80c72570c7f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.normalization.LayerNorm'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "FLOPs: 18065915904.0\n",
      "Parameters: 35322112.0\n"
     ]
    }
   ],
   "source": [
    "#实际计算---FLOPs为18065915904.0, 约为1.8e10, 计算量更少的原因可能是实际前向传播时有一些计算优化。\n",
    "from thop import profile\n",
    "model = DecoderOnlyTransformer(bos_id = start_id, eos_id = end_id, pad_id=pad_id, d_model=d_model, n_heads=n_heads, d_hidden=d_hidden, drop_prob=drop_prob1, voc_size=voc_size, seq_len=seq_len, n_layers=n_layers)\n",
    "model_checkpoint_path = 'checkpoint_steps_9800.pth'\n",
    "checkpoint = torch.load(model_checkpoint_path)\n",
    "model.to('cpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "input_tensor = torch.randint(20, 3485, (1, 512)).long() #(batch_size, seq_len)\n",
    "flops, params = profile(model, inputs=(input_tensor,))\n",
    "print(f\"FLOPs: {flops}\")\n",
    "print(f\"Parameters: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000572e-547a-4a09-9373-56d5a700d057",
   "metadata": {},
   "source": [
    "8. 总结：\n",
    "\n",
    "- 效果不是特别好，可能因为训练数据过少，导致模型输出的单词没有很强的语义连贯性\n",
    "- 可能beam_search的推理方案更适用于本实验\n",
    "- 总体来讲模型学到了古英语风格（使用thy, thou等词而不是your, you）\n",
    "- 关于超参：尝试学习率1e-5, 1e-4, 1e-3, 发现1e-3似乎更好一些"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
